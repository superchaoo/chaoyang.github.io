<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MoMuSE</title>
    <link rel="stylesheet" href="styles.css">
</head>

<body>
    <div class="container">
        <header>
            <h1>MoMuSE: Momentum Multi-modal Target Speaker Extraction for Scenarios with Impaired Visual Cues</h1>
            <p class="authors">Junjie Li<sup>4</sup>, Ke Zhang<sup>1</sup>, Shuai Wang<sup>1</sup>, Haizhou
                Li<sup>1,2,3</sup>, Kong Aik Lee<sup>4</sup></p>
            <p class="affiliations">
                <sup>1</sup>Shenzhen Research Institute of Big Data, Shenzhen, China<br>
                <sup>2</sup>The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen), China<br>
                <sup>3</sup>Department of Electrical and Computer Engineering, National University of Singapore,
                Singapore<br>
                <sup>4</sup>Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic
                University<br>
            </p>
            <p class="contact">Submitted to SPL | Email: <a
                    href="mailto:junjie98.li@connect.polyu.hk">junjie98.li@connect.polyu.hk</a></p>
            <p> Code will be available after this paper is accepted.</p>
        </header>

        <section class="abstract">
            <h2>1. Abstract</h2>
            <p>Audio-visual Target Speaker Extraction (TSE) algorithms outperform audio-only counterparts by integrating
                visual cues. However, real-world scenarios often lack available visual cues due to impairments,
                undermining the effectiveness of audio-visual TSE. Despite this challenge, humans exhibit the ability to
                sustain attentional momentum over time, even when they cannot see the target speaker.
                In this paper, we introduce <b>Momentum Multi-modal target Speaker Extraction</b> (MoMuSE), which
                retains a speaker embedding in memory, enabling the model to continuously track the target speaker.
                MoMuSE is crafted for real-time inference, extracting the current speech window with guidance from both
                visual cues and the speaker embedding. This approach aims to mitigate the deterioration of extraction in
                situations involving impaired videos. Experimental results demonstrate that MoMuSE exhibits significant
                improvement, particularly in scenarios with severe impairment of visual cues.</p>
        </section>

        <section>
            <h2>2. Review of MuSE</h2>
            <img src="./image/MuSE.jpg" width="60%" alt="Model Diagram">
            <p>
                <a href="https://arxiv.org/pdf/2010.07775">MuSE</a><sup>[1]</sup> consists of four modules, like Fig.1
                (a).
            </p>
        </section>

        <section>
            <h2>
                3. Proposed method: MoMuSE
            </h2>
            <img src="./image/MoMuSE.jpg" width="60%" alt="Model Diagram">
            <p>
                Compared to MuSE, we only add a <b>memory bank</b> and a <b>ASEU</b> module. Memory bank is used to
                store the anchor speaker embedding.
                The ASEU module is used to 1. fuse the anchor speaker embedding and current speakr embedding; 2. update
                and anchor speaker embedding.
            </p>
        </section>

        <section class="videos">
            <h2>4. Videos</h2>
                <table>
                    <thead>
                        <tr>
                        <th style="text-align: center">Mixture</th>
                        <th style="text-align: center">ImaginNet/th>
                        <th style="text-align: center">MuSE</th>
                        <th style="text-align: center">MoMuSE</th>
                        </tr>
                    </thead>
                <tbody>
                    <tr>
                    <td style="text-align: center"><video controls ><source src="videos/Mix_M_M.mp4" >Your browser does not support the audio element.</video></td>
                    <td style="text-align: center"><video controls><source src="" >Your browser does not support the audio element.</video></td>
                    <td style="text-align: center"><video controls ><source src="videos/Mix_M_M_S1_MuSE.mp4" >Your browser does not support the audio element.</video></td>
                    <td style="text-align: center"><video controls ><source src="videos/Mix_M_M_S1.mp4" >Your browser does not support the audio element.</video></td>
                    </tr>
                    <tr>
                        <td style="text-align: center"><video controls ><source src="videos/Mix_M_M.mp4" >Your browser does not support the audio element.</video></td>
                        <td style="text-align: center"><video controls><source src="" >Your browser does not support the audio element.</video></td>
                        <td style="text-align: center"><video controls ><source src="videos/Mix_M_M_S2_MuSE.mp4" >Your browser does not support the audio element.</video></td>
                        <td style="text-align: center"><video controls ><source src="videos/Mix_M_M_S2.mp4" >Your browser does not support the audio element.</video></td>
                        </tr>
                </tbody>
            </table>
                
        </section>
        <cite> [1] Pan Z, Tao R, Xu C, et al. Muse: Multi-modal target speaker extraction with visual cues[C]//ICASSP
            2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021:
            6678-6682.</cite>
    </div>
</body>

</html>